{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3facfda2",
   "metadata": {},
   "source": [
    "# Обучение NLP-пайплайна (TF-IDF и K-Means)\n",
    "\n",
    "**Цель:** Создать и сохранить готовый к использованию NLP-пайплайн для анализа и кластеризации логов.\n",
    "\n",
    "## Введение: Как найти смысл в хаосе логов?\n",
    "\n",
    "Логи — это неструктурированный текст. Чтобы машина могла их анализировать, нам нужно превратить их в числа, сохранив при этом их семантический смысл. После этого мы можем применить алгоритмы кластеризации, чтобы автоматически найти группы похожих ошибок.\n",
    "\n",
    "Наш пайплайн состоит из двух этапов:\n",
    "\n",
    "1.  **Векторизация (TF-IDF):** Создание \"карты мира\", где каждое слово имеет свои координаты.\n",
    "2.  **Кластеризация (K-Means):** Нахождение \"столиц\" (центров групп ошибок) на этой карте.\n",
    "\n",
    "Мы обучим обе модели и сохраним их как `tfidf_vectorizer.pkl` и `kmeans_model.pkl`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec85416a",
   "metadata": {},
   "source": [
    "## Шаг 1: Подготовка и генерация данных\n",
    "\n",
    "В реальной системе мы бы взяли сотни тысяч логов из продакшена за последний месяц, чтобы создать репрезентативный корпус текстов. Для демонстрации мы сгенерируем небольшой, но разнообразный набор типовых ошибок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35bd692a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Генерация большого набора логов для обучения NLP-пайплайна...\n",
      "Создан обучающий корпус из 800 лог-сообщений.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "\n",
    "MODELS_DIR = \"trained_models\"\n",
    "if not os.path.exists(MODELS_DIR):\n",
    "    os.makedirs(MODELS_DIR)\n",
    "\n",
    "print(\"Генерация большого набора логов для обучения NLP-пайплайна...\")\n",
    "logs_corpus = [\n",
    "    \"Database connection refused by host 127.0.0.1\",\n",
    "    \"Failed to authenticate user 'guest'\",\n",
    "    \"java.lang.NullPointerException: Cannot invoke 'user.getName()' because 'user' is null\",\n",
    "    \"OutOfMemoryError: Java heap space\",\n",
    "    \"[ERROR] DB Connection timeout after 3000ms\",\n",
    "    \"Invalid API key provided for user 12345\",\n",
    "    \"User authentication failed for admin from IP 192.168.1.1\",\n",
    "    \"Could not connect to Redis server on port 6379\"\n",
    "] * 100\n",
    "\n",
    "print(f\"Создан обучающий корпус из {len(logs_corpus)} лог-сообщений.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6b1595",
   "metadata": {},
   "source": [
    "## Шаг 2: Обучение TF-IDF Векторизатора (`tfidf_vectorizer.pkl`)\n",
    "\n",
    "**TF-IDF (Term Frequency-Inverse Document Frequency)** придает больший вес словам, которые редки во всем наборе данных, но часто встречаются в конкретном сообщении, что идеально для выделения ключевых слов в ошибках.\n",
    "\n",
    "Результатом будет обученный \"словарь\" и математическая модель для преобразования любого нового лога в вектор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2907512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение векторизатора 'tfidf_vectorizer.pkl'...\n",
      " -> Векторизатор TF-IDF обучен на 37 уникальных словах и сохранен в 'trained_models\\tfidf_vectorizer.pkl'\n"
     ]
    }
   ],
   "source": [
    "print(\"Обучение векторизатора 'tfidf_vectorizer.pkl'...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "\n",
    "# Обучаем векторизатор на нашем корпусе и сразу трансформируем данные для следующего шага\n",
    "X_train = tfidf_vectorizer.fit_transform(logs_corpus)\n",
    "\n",
    "vectorizer_path = os.path.join(MODELS_DIR, \"tfidf_vectorizer.pkl\")\n",
    "with open(vectorizer_path, 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "\n",
    "print(f\" -> Векторизатор TF-IDF обучен на {len(tfidf_vectorizer.get_feature_names_out())} уникальных словах и сохранен в '{vectorizer_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bd5883",
   "metadata": {},
   "source": [
    "## Шаг 3: Обучение K-Means Кластеризатора (`kmeans_model.pkl`)\n",
    "\n",
    "**K-Means** — это алгоритм, который находит центры групп (кластеров) на \"карте\", созданной TF-IDF. Обученная модель сможет для любого нового лога (представленного в виде вектора) мгновенно сказать, к какой из заранее определенных групп ошибок он относится."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f55e27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение модели 'kmeans_model.pkl'...\n",
      " -> Модель K-Means обучена на 3 кластера и сохранена в 'trained_models\\kmeans_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "print(\"Обучение модели 'kmeans_model.pkl'...\")\n",
    "# В реальной системе количество кластеров (n_clusters) подбирается экспериментально\n",
    "N_CLUSTERS = 3\n",
    "kmeans_model = KMeans(n_clusters=N_CLUSTERS, random_state=42, n_init='auto')\n",
    "\n",
    "# Обучаем модель на векторизованном корпусе\n",
    "kmeans_model.fit(X_train)\n",
    "\n",
    "kmeans_path = os.path.join(MODELS_DIR, \"kmeans_model.pkl\")\n",
    "with open(kmeans_path, 'wb') as f:\n",
    "    pickle.dump(kmeans_model, f)\n",
    "\n",
    "print(f\" -> Модель K-Means обучена на {N_CLUSTERS} кластера и сохранена в '{kmeans_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b943704",
   "metadata": {},
   "source": [
    "## Заключение\n",
    "\n",
    "**Процесс обучения завершен.**\n",
    "\n",
    "В директории `trained_models/` теперь находятся все необходимые, предварительно обученные артефакты. Основное приложение может теперь загружать эти файлы и выполнять анализ новых данных быстро и эффективно."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
